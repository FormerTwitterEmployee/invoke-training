from typing import Literal

from invoke_training.config.pipelines.finetune_lora_config import LoRATrainingConfig
from invoke_training.config.shared.data.data_loader_config import ImagePairPreferenceSDDataLoaderConfig
from invoke_training.config.shared.optimizer.optimizer_config import OptimizerConfig


class DirectPreferenceOptimizationLoRASDConfig(LoRATrainingConfig):
    type: Literal["DIRECT_PREFERENCE_OPTIMIZATION_LORA_SD"] = "DIRECT_PREFERENCE_OPTIMIZATION_LORA_SD"
    optimizer: OptimizerConfig
    data_loader: ImagePairPreferenceSDDataLoaderConfig

    initial_lora: str | None = None
    """The LoRA checkpoint directory to initialize the LoRA weights from.

    If set, the following configuration parameters are ignored:
    - `train_unet`: The UNet will be trained if it is present in `initial_lora`.
    - `train_text_encoder`: The text encoder will be trained if it is present in `initial_lora`.
    - `lora_rank_dim`: The LoRA rank dimension from `initial_lora` will be used.

    Currently only LoRA checkpoints in the internal `invoke-training` PEFT format are supported (i.e. checkpoints
    generated by an `invoke-training` training pipeline).
    """

    beta: float = 5000.0
    """The beta parameter, as defined in (https://arxiv.org/pdf/2311.12908.pdf). Larger beta values increase the
    KL-Divergence penalty, discouraging divergence from the reference model weights.

    Typical values for `beta` are in the range [1000.0, 10000.0].
    """
