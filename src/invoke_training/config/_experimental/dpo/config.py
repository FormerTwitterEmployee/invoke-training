from typing import Annotated, Literal, Union

from pydantic import Field

from invoke_training.config.pipelines.finetune_lora_config import LoRATrainingConfig
from invoke_training.config.shared.config_base_model import ConfigBaseModel
from invoke_training.config.shared.data.transform_config import SDImageTransformConfig


class HFHubImagePairPreferenceDatasetConfig(ConfigBaseModel):
    type: Literal["HF_HUB_IMAGE_PAIR_PREFERENCE_DATASET"] = "HF_HUB_IMAGE_PAIR_PREFERENCE_DATASET"

    # TODO(ryand): Fill this out.


class ImagePairPreferenceDatasetConfig(ConfigBaseModel):
    type: Literal["IMAGE_PAIR_PREFERENCE_DATASET"] = "IMAGE_PAIR_PREFERENCE_DATASET"

    dataset_dir: str
    """The directory to load the dataset from."""


class ImagePairPreferenceSDDataLoaderConfig(ConfigBaseModel):
    type: Literal["IMAGE_PAIR_PREFERENCE_SD_DATA_LOADER"] = "IMAGE_PAIR_PREFERENCE_SD_DATA_LOADER"

    dataset: Annotated[
        Union[HFHubImagePairPreferenceDatasetConfig, ImagePairPreferenceDatasetConfig], Field(discriminator="type")
    ]

    image_transforms: SDImageTransformConfig

    dataloader_num_workers: int = 0
    """Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process.
    """


class DirectPreferenceOptimizationLoRASDConfig(LoRATrainingConfig):
    type: Literal["DIRECT_PREFERENCE_OPTIMIZATION_LORA_SD"] = "DIRECT_PREFERENCE_OPTIMIZATION_LORA_SD"

    data_loader: ImagePairPreferenceSDDataLoaderConfig

    initial_lora: str | None = None
    """The LoRA checkpoint directory to initialize the LoRA weights from.

    If set, the following configuration parameters are ignored:
    - `train_unet`: The UNet will be trained if it is present in `initial_lora`.
    - `train_text_encoder`: The text encoder will be trained if it is present in `initial_lora`.
    - `lora_rank_dim`: The LoRA rank dimension from `initial_lora` will be used.

    Currently only LoRA checkpoints in the internal `invoke-training` PEFT format are supported (i.e. checkpoints
    generated by an `invoke-training` training pipeline).
    """

    beta: float = 5000.0
    """The beta parameter, as defined in (https://arxiv.org/pdf/2311.12908.pdf). Larger beta values increase the
    KL-Divergence penalty, discouraging divergence from the reference model weights.

    Typical values for `beta` are in the range [1000.0, 10000.0].
    """
