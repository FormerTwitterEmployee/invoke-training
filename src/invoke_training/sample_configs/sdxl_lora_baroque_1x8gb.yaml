# Training mode: Finetuning with LoRA
# Base model:    SDXL 1.0
# Dataset:       https://huggingface.co/datasets/InvokeAI/nga-baroque
# GPU:           1 x 8GB

# Instructions:
# 1. Download the dataset from https://huggingface.co/datasets/InvokeAI/nga-baroque.
# 2. Update the `jsonl_path` field in the `data_loader` section to point to the `metadata.jsonl` file of the downloaded
# dataset.

# Notes:
# This config file has been optimized for 2 primary goals:
#   - Minimize VRAM usage so that an SDXL model can be trained with only 8GB of VRAM.
#   - Achieve reasonable results *quickly* for demo purposes.

type: SDXL_LORA
seed: 1
base_output_dir: output/baroque/sdxl_lora

optimizer:
  optimizer_type: Prodigy
  learning_rate: 1.0
  weight_decay: 0.01
  use_bias_correction: True
  safeguard_warmup: True

data_loader:
  type: IMAGE_CAPTION_SD_DATA_LOADER
  dataset:
    type: IMAGE_CAPTION_JSONL_DATASET
    # Update the jsonl_path field to point to the metadata.jsonl file of the downloaded dataset.
    jsonl_path: data/nga-baroque/metadata.jsonl
  # TODO: More optimizations are needed to train at full 1024x1024 resolution with 8GB VRAM.
  resolution: 512
  # aspect_ratio_buckets:
  #   target_resolution: 1024
  #   start_dim: 512
  #   end_dim: 1536
  #   divisible_by: 128
  caption_prefix: "A baroque painting of"

# General
model: stabilityai/stable-diffusion-xl-base-1.0
vae_model: madebyollin/sdxl-vae-fp16-fix
train_text_encoder: False
cache_text_encoder_outputs: True
enable_cpu_offload_during_validation: True
gradient_accumulation_steps: 4
weight_dtype: bfloat16
gradient_checkpointing: True

max_train_epochs: 6
save_every_n_epochs: 1
validate_every_n_epochs: 1

max_checkpoints: 5
validation_prompts:
  - A baroque painting of a woman carrying a basket of fruit.
  - A baroque painting of a cute Yoda creature.
train_batch_size: 1
num_validation_images_per_prompt: 3
