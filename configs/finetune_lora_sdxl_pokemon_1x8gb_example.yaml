# Training mode: Finetuning with LoRA
# Base model:    SDXL 1.0
# Dataset:       Pokemon
# GPU:           1 x 8GB

# Notes:
# This config file has been optimized for 2 primary goals:
#   - Minimize VRAM usage so that an SDXL model can be trained with only 8GB of VRAM.
#   - Achieve reasonable results *quickly* (<15mins) for demo purposes.
type: FINETUNE_LORA_SDXL
seed: 1
output:
  base_output_dir: output/

optimizer:
  learning_rate: 1.0

  optimizer:
    optimizer_type: Prodigy
    weight_decay: 0.01
    use_bias_correction: True
    safeguard_warmup: True

data_loader:
  type: IMAGE_CAPTION_SD_DATA_LOADER
  dataset:
    type: HF_HUB_IMAGE_CAPTION_DATASET
    dataset_name: lambdalabs/pokemon-blip-captions
  image_transforms:
    resolution: 512

# General
model: stabilityai/stable-diffusion-xl-base-1.0
vae_model: madebyollin/sdxl-vae-fp16-fix
train_text_encoder: False
cache_text_encoder_outputs: True
enable_cpu_offload_during_validation: True
gradient_accumulation_steps: 4
mixed_precision: fp16
xformers: True
gradient_checkpointing: True
# Dataset size is 833. Set max_train_steps to train for 2 epochs.
# ceil(833 / 4) * 3
max_train_steps: 627
save_every_n_epochs: 1
save_every_n_steps: null
max_checkpoints: 100
validation_prompts:
  - A cute yoda pokemon creature.
  - A cute astronaut pokemon creature.
validate_every_n_epochs: 1
train_batch_size: 1
num_validation_images_per_prompt: 3
