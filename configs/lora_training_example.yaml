# This is a sample config for training a Pokemon LoRA model.

output:
  base_output_dir: output/

optimizer:
  learning_rate: 5.0e-4

dataset:
  dataset_name: lambdalabs/pokemon-blip-captions

# General
seed: 1
gradient_accumulation_steps: 1
mixed_precision: fp16
xformers: True
gradient_checkpointing: True
max_train_steps: 4000
save_every_n_epochs: 1
save_every_n_steps: null
max_checkpoints: 100
validation_prompts:
  - yoda
  - astronaut
  - yoda in a space suit
validate_every_n_epochs: 1
train_batch_size: 4
